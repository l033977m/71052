# -*- coding: utf-8 -*-
"""Revised Emotion analysis and prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NcHM2E4Pl85-ftbkBPfsll2kB0D85jPQ
"""

from google.colab import drive
drive.mount('/content/drive')
!ls "/content/drive"

"""# Importing libraries"""

import re
import nltk
import string
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, Conv1D, MaxPooling1D, GRU, Reshape, Flatten

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))
lemmatizer= WordNetLemmatizer()

# Modelling
from sklearn.model_selection import train_test_split,KFold, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,confusion_matrix, classification_report
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score
from sklearn.svm import SVC

#Lime
!pip install lime
from lime import lime_text
from lime.lime_text import LimeTextExplainer
from lime.lime_text import IndexedString,IndexedCharacters
from lime.lime_base import LimeBase
from lime.lime_text import explanation
sns.set(font_scale=1.3)
nltk.download('omw-1.4')

"""Sample data set creation"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Colab Notebooks/data
df_go = pd.read_csv('go_emotions.txt', names=['Text', 'Emotion'], sep=';')
df_w = pd.read_csv('df_w.csv', names=['Text', 'Emotion'], sep=';')

print("Shape of concatenated DataFrame:", df_w.shape)
print(df_w.shape)
emotion_counts = df_go['Emotion'].value_counts()
print("Emotion count for go", emotion_counts)
emotion_counts = df_w['Emotion'].value_counts()
print("Emotion count for full", emotion_counts)

# Extract data from two data sets and create balanced labelled dataset
# Filter rows with emotion "joy"
df_w_joy = df_w[df_w['Emotion'] == 'joy']
sampled_w_joy = df_w_joy.sample(n=3000, random_state=47)

# Filter rows with emotion "sadness"
df_w_sad = df_w[df_w['Emotion'] == 'sadness']
sampled_w_sad = df_w_sad.sample(n=2600, random_state=47)

# Filter rows with emotion "anger"
df_w_anger = df_w[df_w['Emotion'] == 'anger']
sampled_w_anger = df_w_anger.sample(n=2600, random_state=47)

# Filter rows with emotion "fear"
df_w_fear = df_w[df_w['Emotion'] == 'fear']
sampled_w_fear = df_w_fear.sample(n=2373, random_state=47)

# Filter rows with emotion "surprise"
df_w_surprise = df_go[df_go['Emotion'] == 'surprise']
sampled_w_surprise = df_w_surprise.sample(n=1781, random_state=47)

# Filter rows with emotion "disgust"
df_w_disgust = df_go[df_go['Emotion'] == 'disgust']
sampled_w_disgust = df_w_disgust.sample(n=2300, random_state=47)

# Filter rows with emotion "fear"
df_w_fear_g = df_go[df_go['Emotion'] == 'fear']
sampled_w_fear_g = df_w_fear_g.sample(n=200, random_state=47)

# Filter rows with emotion "sadness"
df_w_sad_g = df_go[df_go['Emotion'] == 'sadness']
sampled_w_sad_g = df_w_sad_g.sample(n=1250, random_state=47)

# Filter rows with emotion "love"
df_w_love = df_w[df_w['Emotion'] == 'love']
# Filter rows with emotion "surprise"
df_w_surprise = df_w[df_w['Emotion'] == 'surprise']

df_full = pd.concat([sampled_w_joy, sampled_w_sad, sampled_w_anger, sampled_w_fear, sampled_w_surprise, sampled_w_disgust, df_w_love, df_w_surprise, sampled_w_fear_g], ignore_index=True)

import matplotlib.pyplot as plt

# Assuming emotion_counts is a Pandas Series containing the counts of different emotions
# You can replace it with your actual emotion counts data
emotion_counts = df_full['Emotion'].value_counts()

# Plotting the counts of different emotions
plt.figure(figsize=(10, 6))
emotion_counts.plot(kind='bar', color='skyblue')
plt.title('Counts of Different Emotions')
plt.xlabel('Emotion')
plt.ylabel('Count')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

"""## Cleaning and Preprocessing"""

#removing duplicated text
index = df_full[df_full['Text'].duplicated() == True].index
df_full.drop(index, axis = 0, inplace = True)
df_full.reset_index(inplace=True, drop = True)

emotion_counts = df_full['Emotion'].value_counts()
print("Emotion count for full", emotion_counts)
print("Shape", df_full.shape)
df_full.head()

# @title Emotion

from matplotlib import pyplot as plt
import seaborn as sns
df_full.groupby('Emotion').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

def lemmatization(text):
    lemmatizer= WordNetLemmatizer()

    text = text.split()

    text=[lemmatizer.lemmatize(y) for y in text]

    return " " .join(text)

def remove_stop_words(text):

    Text=[i for i in str(text).split() if i not in stop_words]
    return " ".join(Text)

def Removing_numbers(text):
    text=''.join([i for i in text if not i.isdigit()])
    return text

def lower_case(text):

    text = text.split()

    text=[y.lower() for y in text]

    return " " .join(text)

def Removing_punctuations(text):
    ## Remove punctuations
    text = re.sub('[%s]' % re.escape("""!"#$%&'()*+,،-./:;<=>؟?@[\]^_`{|}~"""), ' ', text)
    text = text.replace('؛',"", )

    ## remove extra whitespace
    text = re.sub('\s+', ' ', text)
    text =  " ".join(text.split())
    return text.strip()

def Removing_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

def remove_small_sentences(df):
    for i in range(len(df)):
        if len(df.text.iloc[i].split()) < 3:
            df.text.iloc[i] = np.nan

def normalize_text(df):
    df.Text=df.Text.apply(lambda text : lower_case(text))
    df.Text=df.Text.apply(lambda text : remove_stop_words(text))
    df.Text=df.Text.apply(lambda text : Removing_numbers(text))
    df.Text=df.Text.apply(lambda text : Removing_punctuations(text))
    df.Text=df.Text.apply(lambda text : Removing_urls(text))
    df.Text=df.Text.apply(lambda text : lemmatization(text))
    return df

def normalized_sentence(sentence):
    sentence= lower_case(sentence)
    sentence= remove_stop_words(sentence)
    sentence= Removing_numbers(sentence)
    sentence= Removing_punctuations(sentence)
    sentence= Removing_urls(sentence)
    sentence= lemmatization(sentence)
    return sentence

def normalize_sentences(sentences):
    normalized_sentences = []
    for sentence in sentences:
        normalize_sentence = normalized_sentence(sentence)
        normalized_sentences.append(normalize_sentence)
    return normalized_sentences

nltk.download("wordnet")
normalized_sentence("My Name is Udesh. @Tweets,  plays 2024")

"""Dataset splitting"""

# Split the dataset into train, test, and validation sets
train_df, test_val_df = train_test_split(df_full, test_size=0.3, random_state=42)
test_df, val_df = train_test_split(test_val_df, test_size=0.5, random_state=42)

df_train = train_df
df_val = val_df
df_test = test_df

df_train= normalize_text(df_train)
df_test= normalize_text(df_test)
df_val= normalize_text(df_val)

"""## Tokenization, encoding and padding"""

#Splitting the text from the labels
X_train = df_train['Text']
y_train = df_train['Emotion']

X_test = df_test['Text']
y_test = df_test['Emotion']

X_val = df_val['Text']
y_val = df_val['Emotion']
print(set(y_train))
print(set(y_test))
print(set(y_val))

# Encode labels
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)
y_val = le.transform(y_val)
#print the labels after encoding
print(set(y_train))

#Convert the class vector (integers) to binary class matrix
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
y_val = to_categorical(y_val)

# Tokenize words
tokenizer = Tokenizer(oov_token='UNK')
tokenizer.fit_on_texts(pd.concat([X_train, X_test], axis=0))

sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)
sequences_val = tokenizer.texts_to_sequences(X_val)

maxlen = max([len(t) for t in df_train['Text']])
maxlen

X_train = pad_sequences(sequences_train, maxlen=229, truncating='pre')
X_test = pad_sequences(sequences_test, maxlen=229, truncating='pre')
X_val = pad_sequences(sequences_val, maxlen=229, truncating='pre')

vocabSize = len(tokenizer.index_word) + 1
print(f"Vocabulary size = {vocabSize}")

"""## Word Embedding"""



def load_glove_embeddings(path_to_glove_file, vocab_size, embedding_dim, tokenizer):
    """
    Load GloVe word embeddings and assign them to the vocabulary.

    Args:
    - path_to_glove_file (str): Path to the GloVe embeddings file.
    - vocab_size (int): Size of the vocabulary.
    - embedding_dim (int): Dimensionality of the word embeddings.
    - tokenizer (Tokenizer): Tokenizer object used to tokenize text data.

    Returns:
    - embedding_matrix (np.ndarray): Matrix containing word embeddings for the vocabulary.
    - hits (int): Number of words found in the GloVe embeddings.
    - misses (int): Number of words not found in the GloVe embeddings.
    """
    embeddings_index = {}
    hits = 0
    misses = 0

    # Read word vectors
    with open(path_to_glove_file) as f:
        for line in f:
            word, coefs = line.split(maxsplit=1)
            coefs = np.fromstring(coefs, "f", sep=" ")
            embeddings_index[word] = coefs

    print("Found %s word vectors." % len(embeddings_index))

    # Assign word vectors to our dictionary/vocabulary
    embedding_matrix = np.zeros((vocab_size, embedding_dim))
    for word, i in tokenizer.word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
            hits += 1
        else:
            misses += 1

    print("Converted %d words (%d misses)" % (hits, misses))
    return embedding_matrix, hits, misses

# Example usage
path_to_glove_file = 'glove.twitter.27B.200d.txt'
num_tokens = vocabSize
embedding_dim = 200

embedding_matrix, hits, misses = load_glove_embeddings(path_to_glove_file, num_tokens, embedding_dim, tokenizer)

"""## Modeling"""

# Build neural network architecture

from keras.models import Sequential
from keras.layers import Embedding, Bidirectional, LSTM, Conv1D, Dense
from keras.optimizers import Adam

# Build neural network architecture
adam = Adam(learning_rate=0.005)

model = Sequential()
model.add(Embedding(vocabSize, 200, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=False))

model.add(Bidirectional(LSTM(128, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)))
model.add(Conv1D(filters=256, kernel_size=5, activation='relu'))
model.add(GRU(128, dropout=0.4, recurrent_dropout=0.4))
model.add(Dense(7, activation='softmax'))

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

from keras.utils import plot_model
plot_model(model)

#to stop the training when the loss starts to increase
callback = EarlyStopping(
    monitor="val_loss",
    patience=5,
    restore_best_weights=True,
)

# Fit model
history = model.fit(X_train,
                    y_train,
                    validation_data=(X_val, y_val),
                    verbose=1,
                    batch_size=128,
                    epochs=20,
                    callbacks=[callback],
                    shuffle=True
                   )

#print the overall loss and accuracy
model.evaluate(X_val, y_val, verbose=1)

#print the overall loss and accuracy
model.evaluate(X_test, y_test, verbose=1)

predicted = model.predict(X_test)
y_pred = predicted.argmax(axis=-1)

print(classification_report(le.transform(df_test['Emotion']), y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Convert emotion labels to numerical labels
y_true = le.transform(df_test['Emotion'])

# Generate confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot confusion matrix as heatmap
plt.figure(figsize=(7, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))


plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')  # Add x axis label
plt.ylabel('Accuracy')  # Add y axis label
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')  # Add x axis label
plt.ylabel('Loss')  # Add y axis label
plt.legend()

plt.show()

# Classify custom sample

class_names = le.classes_

sentences = [
            "Running is a divine feeling for him",
            "I feel sorry for him",
            "I watched a horror movie",
            ]

for sentence in sentences:
    sentence1 = sentence
    print(sentence1)
    sentence = normalized_sentence(sentence)
    sentence = tokenizer.texts_to_sequences([sentence])
    sentence = pad_sequences(sentence, maxlen=229, truncating='pre')
    result = le.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]
    proba =  np.max(model.predict(sentence))
    print(f"{result} : {proba}\n\n")


from lime.lime_text import LimeTextExplainer
import matplotlib.pyplot as plt

# Define a function to predict class probabilities
def predict_proba(sentences):
    sequences = normalize_sentences(sentences)
    sequences = tokenizer.texts_to_sequences(sentences)
    sequences = pad_sequences(sequences, maxlen=229, truncating='pre')
    return model.predict(sequences)

# Use LIME to explain the predictions and visualize the explanations
explainer = LimeTextExplainer(class_names=class_names)
for sentence in sentences:
    sentence = str(sentence)  # Ensure sentence is a string
    print(sentence)

    # Explain prediction
    exp = explainer.explain_instance(sentence, predict_proba, num_features=10, top_labels=3)
    exp.show_in_notebook(text=True)
    plt.show()
    print("\n")

sentence= 'I feel awesome today'
print(sentence)
sentence = normalized_sentence(sentence)
sentence = tokenizer.texts_to_sequences([sentence])
sentence = pad_sequences(sentence, maxlen=229, truncating='pre')
result = le.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]
proba =  np.max(model.predict(sentence))
print(f"{result} : {proba}\n\n")
# Get the predicted labels and probabilities
predictions = model.predict(sentence)
sorted_indices = np.argsort(-predictions, axis=-1)
predicted_labels = le.inverse_transform(sorted_indices.reshape(-1))
predicted_probabilities = np.sort(predictions, axis=-1)[:, ::-1]
# Loop through each prediction
for i in range(len(predictions)):
    for j, index in enumerate(sorted_indices[i]):
        label = le.inverse_transform(index.reshape(-1))[0]
        probability = predicted_probabilities[i][j]
        print(f"{label} : {probability*100}%")
    print()

# from joblib import dump

# # Save LabelEncoder to a file
# dump(le, 'label_encoder.joblib')
# dump(tokenizer, 'tokenizer.joblib')

# model.save('BiLSTM-CNN-GRU.h5')